{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14145187",
   "metadata": {},
   "source": [
    "# üî® Project 1 ‚Äì Text Cleaner\n",
    "- Building a reusable, modular text preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d352ba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b0bcd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\oumme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\oumme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\oumme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\oumme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure the required NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "659bcf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize required components\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans input text string using the following steps:\n",
    "    1. Lowercasing\n",
    "    2. Removing HTML tags\n",
    "    3. Removing URLs\n",
    "    4. Removing punctuation\n",
    "    5. Removing numbers\n",
    "    6. Tokenization\n",
    "    7. Stopword removal\n",
    "    8. Lemmatization\n",
    "    Returns: List of clean tokens\n",
    "    \"\"\"\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # 3. Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # 4. Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # 5. Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 6. Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 7. Remove stopwords\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # 8. Lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    \n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d381b6",
   "metadata": {},
   "source": [
    "### Let's Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2f16e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'aged', 'jumped', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "sample = \"The <b>quick</b> brown fox, aged 5, jumped over http://example.com the lazy dog!\"\n",
    "print(clean_text(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393224fe",
   "metadata": {},
   "source": [
    "### Now lets enhance it a bit more by adding customizations and enhancing it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47cbde6",
   "metadata": {},
   "source": [
    "- Contraction removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e29ad539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions  # NEW\n",
    "\n",
    "def clean_text(text,\n",
    "               lowercase=True,\n",
    "               expand_contractions=True,  # NEW\n",
    "               remove_html=True,\n",
    "               remove_urls=True,\n",
    "               remove_punct=True,\n",
    "               remove_numbers=True,\n",
    "               remove_stopwords=True,\n",
    "               lemmatize=True):\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    if expand_contractions:\n",
    "        text = contractions.fix(text)  # NEW STEP\n",
    "    \n",
    "    if remove_html:\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        \n",
    "    if remove_urls:\n",
    "        text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "        \n",
    "    if remove_punct:\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        \n",
    "    if remove_numbers:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "        \n",
    "    if lemmatize:\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "        \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a75e61fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['going', 'store', 'coming']\n"
     ]
    }
   ],
   "source": [
    "sample = \"She's going to the store, and he isn't coming.\"\n",
    "cleaned = clean_text(sample, expand_contractions=True)\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e2e66f",
   "metadata": {},
   "source": [
    "- Emoji removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0c2b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji  # NEW\n",
    "\n",
    "def clean_text(text,\n",
    "               lowercase=True,\n",
    "               expand_contractions=True,\n",
    "               remove_html=True,\n",
    "               remove_urls=True,\n",
    "               remove_punct=True,\n",
    "               remove_numbers=True,\n",
    "               remove_stopwords=True,\n",
    "               lemmatize=True,\n",
    "               remove_emojis=True,          # NEW\n",
    "               map_emojis_to_text=False):   # NEW\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    if expand_contractions:\n",
    "        text = contractions.fix(text)\n",
    "    \n",
    "    if map_emojis_to_text:\n",
    "        text = emoji.demojize(text)  # üòÄ ‚Üí :grinning_face:\n",
    "    elif remove_emojis:\n",
    "        text = emoji.replace_emoji(text, replace='')\n",
    "    \n",
    "    if remove_html:\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        \n",
    "    if remove_urls:\n",
    "        text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "        \n",
    "    if remove_punct:\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        \n",
    "    if remove_numbers:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "        \n",
    "    if lemmatize:\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2391f703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'pizza', 'face_savoring_food', 'exam', 'make', 'weary_face']\n"
     ]
    }
   ],
   "source": [
    "sample = \"I love pizza üòã but exams make me üò©!\"\n",
    "print(clean_text(sample, map_emojis_to_text=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489014f3",
   "metadata": {},
   "source": [
    "- Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce664f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob  # NEW\n",
    "\n",
    "def clean_text(text,\n",
    "               lowercase=True,\n",
    "               expand_contractions=True,\n",
    "               remove_html=True,\n",
    "               remove_urls=True,\n",
    "               remove_punct=True,\n",
    "               remove_numbers=True,\n",
    "               remove_stopwords=True,\n",
    "               lemmatize=True,\n",
    "               remove_emojis=True,\n",
    "               map_emojis_to_text=False,\n",
    "               correct_spelling=False):  # NEW\n",
    "     \n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "\n",
    "    if expand_contractions:\n",
    "        text = contractions.fix(text)\n",
    "\n",
    "    if map_emojis_to_text:\n",
    "        text = emoji.demojize(text)\n",
    "    elif remove_emojis:\n",
    "        text = emoji.replace_emoji(text, replace='')\n",
    "\n",
    "    if remove_html:\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    if remove_urls:\n",
    "        text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "    if remove_punct:\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    if remove_numbers:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    if remove_stopwords:\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "\n",
    "    if lemmatize:\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "    if correct_spelling:\n",
    "        # Join tokens, correct spelling, re-tokenize\n",
    "        blob = TextBlob(\" \".join(tokens))\n",
    "        corrected = blob.correct()\n",
    "        tokens = word_tokenize(str(corrected))\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3702d228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reply', 'love', 'delicious', 'pizzza', 'italy']\n"
     ]
    }
   ],
   "source": [
    "text = \"I reely loveee delicius pizzza üçï in Itly!\"\n",
    "print(clean_text(text, correct_spelling=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3a64f5",
   "metadata": {},
   "source": [
    "- Unicode Normalization & Whitespace Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1180bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def clean_text(text,\n",
    "               lowercase=True,\n",
    "               expand_contractions=True,\n",
    "               remove_html=True,\n",
    "               remove_urls=True,\n",
    "               remove_punct=True,\n",
    "               remove_numbers=True,\n",
    "               remove_stopwords=True,\n",
    "               lemmatize=True,\n",
    "               remove_emojis=True,\n",
    "               map_emojis_to_text=False,\n",
    "               correct_spelling=False):\n",
    "\n",
    "    # --- New Step: Normalize Unicode ---\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    # 1. Unicode normalization & whitespace cleanup\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # 2. Lowercase\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "\n",
    "    # 3. Expand contractions\n",
    "    if expand_contractions:\n",
    "        text = contractions.fix(text)\n",
    "\n",
    "    # 4. Emoji handling\n",
    "    if map_emojis_to_text:\n",
    "        text = emoji.demojize(text)\n",
    "    elif remove_emojis:\n",
    "        text = emoji.replace_emoji(text, replace='')\n",
    "\n",
    "    # 5. Remove HTML tags\n",
    "    if remove_html:\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # 6. Remove URLs\n",
    "    if remove_urls:\n",
    "        text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "    # 7. Remove punctuation\n",
    "    if remove_punct:\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # 8. Remove numbers\n",
    "    if remove_numbers:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # 9. Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 10. Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "\n",
    "    # 11. Lemmatization\n",
    "    if lemmatize:\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "    # 12. Spelling correction (slow)\n",
    "    if correct_spelling and tokens:\n",
    "        blob = TextBlob(\" \".join(tokens))\n",
    "        corrected = blob.correct()\n",
    "        tokens = word_tokenize(str(corrected))\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9f36588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tƒìxt', 'with', 'strange', 'character', 'and', 'odd', 'space', 'newlines']\n"
     ]
    }
   ],
   "source": [
    "weird_text = \"‚ÄúTƒìxt ‚Äù with‚Äî strange\\u200b characters\\tand‚ÄÉodd spaces.\\n\\nNewlines.\"\n",
    "print(clean_text(weird_text, remove_stopwords=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a0c937",
   "metadata": {},
   "source": [
    "### Batch-Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91d9b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()  # Enable progress bar on DataFrame operations\n",
    "\n",
    "def clean_dataframe_column(df, column, **cleaner_kwargs):\n",
    "    \"\"\"\n",
    "    Apply clean_text() to a DataFrame column.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - column: column name containing raw text\n",
    "    - cleaner_kwargs: keyword arguments passed to clean_text()\n",
    "    \n",
    "    Returns:\n",
    "    - df: DataFrame with a new column: column_cleaned\n",
    "    \"\"\"\n",
    "    cleaned_col = f\"{column}_cleaned\"\n",
    "    df[cleaned_col] = df[column].progress_apply(lambda x: clean_text(str(x), **cleaner_kwargs))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4d2c56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 119.24it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'raw_text': [\n",
    "        \"This is a <b>test</b> sentence with a link: https://t.co/abc123 üòä\",\n",
    "        \"Here's another line ‚Äî with emojis üòÇüòÇ and   whitespace!\",\n",
    "        \"The caf√©'s cr√®me br√ªl√©e wasn't bad... at all!\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Clean it!\n",
    "df = clean_dataframe_column(df, 'raw_text', remove_stopwords=True, remove_emojis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4fffc3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>raw_text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a &lt;b&gt;test&lt;/b&gt; sentence with a link: ht...</td>\n",
       "      <td>[test, sentence, link]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Here's another line ‚Äî with emojis üòÇüòÇ and   whi...</td>\n",
       "      <td>[another, line, emojis, whitespace]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The caf√©'s cr√®me br√ªl√©e wasn't bad... at all!</td>\n",
       "      <td>[caf√©s, cr√®me, br√ªl√©e, bad]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_text  \\\n",
       "0  This is a <b>test</b> sentence with a link: ht...   \n",
       "1  Here's another line ‚Äî with emojis üòÇüòÇ and   whi...   \n",
       "2      The caf√©'s cr√®me br√ªl√©e wasn't bad... at all!   \n",
       "\n",
       "                      raw_text_cleaned  \n",
       "0               [test, sentence, link]  \n",
       "1  [another, line, emojis, whitespace]  \n",
       "2          [caf√©s, cr√®me, br√ªl√©e, bad]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d5dbe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
